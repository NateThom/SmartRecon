{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import gc\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import ensemble\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV, cross_validate\n",
    "\n",
    "from statistics import mean\n",
    "import umap\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.ioff()\n",
    "# %matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import wandb\n",
    "# wandb.init(project=\"smart_attacker_same_lightbulb\", entity=\"unr-mpl\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def remove_class(class_name, dataset):\n",
    "    dataset = dataset[dataset[\"class\"] != class_name]\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "path_to_iot_noise_cleaned = \"/home/nthom/Documents/SmartRecon/Fingerprinting in Noisy Network Environments/data/noise/iot_noise/iot_noise_hashes_cleaned.csv\"\n",
    "path_to_iot_noise_uncleaned  = \"/home/nthom/Documents/SmartRecon/Fingerprinting in Noisy Network Environments/data/noise/iot_noise/iot_noise_hashes_uncleaned.csv\"\n",
    "\n",
    "path_to_network_noise_cleaned = \"/home/nthom/Documents/SmartRecon/Fingerprinting in Noisy Network Environments/data/noise/network_noise/network_noise_hashes_cleaned.csv\"\n",
    "path_to_network_noise_uncleaned = \"/home/nthom/Documents/SmartRecon/Fingerprinting in Noisy Network Environments/data/noise/network_noise/network_noise_hashes_uncleaned.csv\"\n",
    "\n",
    "path_to_per_packet_cleaned_devices = \"/home/nthom/Documents/SmartRecon/Fingerprinting in Noisy Network Environments/data/per_packet_hashes/cleaned/per-packet-hashes-cleaned.csv\"\n",
    "path_to_per_packet_uncleaned_devices = \"/home/nthom/Documents/SmartRecon/Fingerprinting in Noisy Network Environments/data/per_packet_hashes/uncleaned/per-packet-hashes-uncleaned.csv\"\n",
    "\n",
    "path_to_per_packet_cleaned_categories = \"/home/nthom/Documents/SmartRecon/Fingerprinting in Noisy Network Environments/data/per_packet_hashes/cleaned/cleaned-categories.csv\"\n",
    "path_to_per_packet_uncleaned_categories = \"/home/nthom/Documents/SmartRecon/Fingerprinting in Noisy Network Environments/data/per_packet_hashes/uncleaned/uncleaned-categories.csv\"\n",
    "\n",
    "path_to_same_plug_cleaned_interaction = \"/home/nthom/Documents/SmartRecon/Fingerprinting in Noisy Network Environments/data/same_device/same_plug/same_plug_cleaned_interaction/\"\n",
    "path_to_same_plug_cleaned_no_interaction = \"/home/nthom/Documents/SmartRecon/Fingerprinting in Noisy Network Environments/data/same_device/same_plug/same_plug_cleaned_no_interaction/\"\n",
    "path_to_same_plug_uncleaned_interaction = \"/home/nthom/Documents/SmartRecon/Fingerprinting in Noisy Network Environments/data/same_device/same_plug/same_plug_uncleaned_interaction/\"\n",
    "path_to_same_plug_uncleaned_no_interaction = \"/home/nthom/Documents/SmartRecon/Fingerprinting in Noisy Network Environments/data/same_device/same_plug/same_plug_uncleaned_no_interaction/\"\n",
    "\n",
    "path_to_same_bulb_cleaned_interaction = \"/home/nthom/Documents/SmartRecon/Fingerprinting in Noisy Network Environments/data/same_device/same_lightbulb/same_lightbulb_cleaned_interaction/\"\n",
    "path_to_same_bulb_cleaned_no_interaction = \"/home/nthom/Documents/SmartRecon/Fingerprinting in Noisy Network Environments/data/same_device/same_lightbulb/same_lightbulb_cleaned_no_interaction/\"\n",
    "path_to_same_bulb_uncleaned_interaction = \"/home/nthom/Documents/SmartRecon/Fingerprinting in Noisy Network Environments/data/same_device/same_lightbulb/same_lightbulb_uncleaned_interaction/\"\n",
    "path_to_same_bulb_uncleaned_no_interaction = \"/home/nthom/Documents/SmartRecon/Fingerprinting in Noisy Network Environments/data/same_device/same_lightbulb/same_lightbulb_uncleaned_no_interaction/\"\n",
    "\n",
    "path_to_same_cam_cleaned_interaction = \"/home/nthom/Documents/SmartRecon/Fingerprinting in Noisy Network Environments/data/same_device/same_cam/same_cam_cleaned_interaction/\"\n",
    "path_to_same_cam_cleaned_no_interaction = \"/home/nthom/Documents/SmartRecon/Fingerprinting in Noisy Network Environments/data/same_device/same_cam/same_cam_cleaned_no_interaction/\"\n",
    "path_to_same_cam_uncleaned_interaction = \"/home/nthom/Documents/SmartRecon/Fingerprinting in Noisy Network Environments/data/same_device/same_cam/same_cam_uncleaned_interaction/\"\n",
    "path_to_same_cam_uncleaned_no_interaction = \"/home/nthom/Documents/SmartRecon/Fingerprinting in Noisy Network Environments/data/same_device/same_cam/same_cam_uncleaned_no_interaction/\"\n",
    "\n",
    "path_to_simhash = \"/home/nthom/Documents/SmartRecon/Fingerprinting in Noisy Network Environments/data/simhashes/\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def noise_generator(original_df, names, one_hundred_times=False):\n",
    "    num_samples_to_generate = len(original_df.index)\n",
    "    if one_hundred_times == True:\n",
    "        num_samples_to_generate *= 100\n",
    "    num_digits_in_hash = 32\n",
    "    label = \"other\"\n",
    "    random_list = []\n",
    "\n",
    "    for i in tqdm(range(num_samples_to_generate), desc=\"Generating random noise\"):\n",
    "        temp_random_hash = []\n",
    "        for j in range(num_digits_in_hash):\n",
    "            temp_random_hash.append(random.randint(0, 255))\n",
    "        temp_random_hash.append(label)\n",
    "        random_list.append(temp_random_hash)\n",
    "\n",
    "    output_list = np.concatenate((np.array(random_list), np.array(original_df.values.tolist())), axis=0).tolist()\n",
    "    output_df = pd.DataFrame(output_list, columns=names)\n",
    "    return output_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garbage collector: collected 0 objects.\n"
     ]
    }
   ],
   "source": [
    "def combine_csv(csv_list, names):\n",
    "    final_df = pd.DataFrame(columns=names)\n",
    "    for index, csv in enumerate(csv_list):\n",
    "        temp_df = pd.read_csv(csv, names=names)\n",
    "        final_df = pd.concat([final_df, temp_df])\n",
    "\n",
    "    return final_df\n",
    "\n",
    "def get_dataset():\n",
    "    names = ['dim1','dim2','dim3','dim4','dim5','dim6','dim7','dim8','dim9','dim10','dim11','dim12','dim13','dim14','dim15','dim16','dim17','dim18','dim19','dim20','dim21','dim22','dim23','dim24','dim25','dim26','dim27','dim28','dim29','dim30','dim31','dim32','class']\n",
    "\n",
    "    experiment_type = int(input(\"Select one of the following: \\n1. Nilsimsa Per-Packet Devices \\n2. Nilsimsa Per-Packet Categories \\n3. Nilsimsa Identical Devices \\n4. SimHash Identical Devices \\n5. 100x Noise\"))\n",
    "    c_uc = int(input(\"Select one of the following: \\n1. Cleaned \\n2. Uncleaned\"))\n",
    "\n",
    "    if experiment_type == 1 and c_uc == 1:\n",
    "        noise = int(input(\"Select one of the following: \\n1. Random \\n2. IoT Cleaned \\n3. IoT Uncleaned \\n4. Network Cleaned \\n5. Network Uncleaned \\n6. None\"))\n",
    "\n",
    "        if noise != 6:\n",
    "\n",
    "            if noise == 1:\n",
    "                dataset = read_csv(path_to_per_packet_cleaned_devices, names=names)\n",
    "                dataset = noise_generator(dataset, names)\n",
    "                name = \"cleaned_devices-random\"\n",
    "            elif noise == 2:\n",
    "                csv_list = [path_to_per_packet_cleaned_devices, path_to_iot_noise_cleaned]\n",
    "                dataset = combine_csv(csv_list, names)\n",
    "                name = \"cleaned_devices-cleaned_iot\"\n",
    "            elif noise == 3:\n",
    "                csv_list = [path_to_per_packet_cleaned_devices, path_to_iot_noise_uncleaned]\n",
    "                dataset = combine_csv(csv_list, names)\n",
    "                name = \"cleaned_devices-uncleaned_iot\"\n",
    "            elif noise == 4:\n",
    "                csv_list = [path_to_per_packet_cleaned_devices, path_to_network_noise_cleaned]\n",
    "                dataset = combine_csv(csv_list, names)\n",
    "                name = \"cleaned_devices-cleaned_network\"\n",
    "            elif noise == 5:\n",
    "                csv_list = [path_to_per_packet_cleaned_devices, path_to_network_noise_uncleaned]\n",
    "                dataset = combine_csv(csv_list, names)\n",
    "                name = \"cleaned_devices-uncleaned_network\"\n",
    "        else:\n",
    "            dataset = read_csv(path_to_per_packet_cleaned_devices, names=names)\n",
    "            name = \"cleaned_devices\"\n",
    "    elif experiment_type == 1 and c_uc == 2:\n",
    "        noise = int(input(\"Select one of the following: \\n1. Random \\n2. IoT Cleaned \\n3. IoT Uncleaned \\n4. Network Cleaned \\n5. Network Uncleaned \\n6. None\"))\n",
    "\n",
    "        if noise != 6:\n",
    "\n",
    "            if noise == 1:\n",
    "                dataset = read_csv(path_to_per_packet_uncleaned_devices, names=names)\n",
    "                dataset = noise_generator(dataset, names)\n",
    "                name = \"uncleaned_devices-random\"\n",
    "            elif noise == 2:\n",
    "                csv_list = [path_to_per_packet_uncleaned_devices, path_to_iot_noise_cleaned]\n",
    "                dataset = combine_csv(csv_list, names)\n",
    "                name = \"uncleaned_devices-cleaned_iot\"\n",
    "            elif noise == 3:\n",
    "                csv_list = [path_to_per_packet_uncleaned_devices, path_to_iot_noise_uncleaned]\n",
    "                dataset = combine_csv(csv_list, names)\n",
    "                name = \"uncleaned_devices-uncleaned_iot\"\n",
    "            elif noise == 4:\n",
    "                csv_list = [path_to_per_packet_uncleaned_devices, path_to_network_noise_cleaned]\n",
    "                dataset = combine_csv(csv_list, names)\n",
    "                name = \"uncleaned_devices-cleaned_network\"\n",
    "            elif noise == 5:\n",
    "                csv_list = [path_to_per_packet_uncleaned_devices, path_to_network_noise_uncleaned]\n",
    "                dataset = combine_csv(csv_list, names)\n",
    "                name = \"uncleaned_devices-uncleaned-network\"\n",
    "        else:\n",
    "            dataset = read_csv(path_to_per_packet_uncleaned_devices, names=names)\n",
    "            name = \"uncleaned_devices\"\n",
    "    elif experiment_type == 2 and c_uc == 1:\n",
    "        noise = int(input(\"Select one of the following: \\n1. Random \\n2. IoT Cleaned \\n3. IoT Uncleaned \\n4. Network Cleaned \\n5. Network Uncleaned \\n6. None\"))\n",
    "\n",
    "        if noise != 6:\n",
    "\n",
    "            if noise == 1:\n",
    "                dataset = read_csv(path_to_per_packet_cleaned_categories, names=names)\n",
    "                dataset = noise_generator(dataset, names)\n",
    "                name = \"cleaned_categories-random\"\n",
    "            elif noise == 2:\n",
    "                csv_list = [path_to_per_packet_cleaned_categories, path_to_iot_noise_cleaned]\n",
    "                dataset = combine_csv(csv_list, names)\n",
    "                name = \"cleaned_categories-cleaned_iot\"\n",
    "            elif noise == 3:\n",
    "                csv_list = [path_to_per_packet_cleaned_categories, path_to_iot_noise_uncleaned]\n",
    "                dataset = combine_csv(csv_list, names)\n",
    "                name = \"cleaned_categories-uncleaned_iot\"\n",
    "            elif noise == 4:\n",
    "                csv_list = [path_to_per_packet_cleaned_categories, path_to_network_noise_cleaned]\n",
    "                dataset = combine_csv(csv_list, names)\n",
    "                name = \"cleaned_categories-cleaned_network\"\n",
    "            elif noise == 5:\n",
    "                csv_list = [path_to_per_packet_cleaned_categories, path_to_network_noise_uncleaned]\n",
    "                dataset = combine_csv(csv_list, names)\n",
    "                name = \"cleaned_categories-uncleaned_network\"\n",
    "        else:\n",
    "            dataset = read_csv(path_to_per_packet_cleaned_categories, names=names)\n",
    "            name = \"cleaned_categories\"\n",
    "    elif experiment_type == 2 and c_uc == 2:\n",
    "        noise = int(input(\"Select one of the following: \\n1. Random \\n2. IoT Cleaned \\n3. IoT Uncleaned \\n4. Network Cleaned \\n5. Network Uncleaned \\n6. None\"))\n",
    "\n",
    "        if noise != 6:\n",
    "\n",
    "            if noise == 1:\n",
    "                dataset = read_csv(path_to_per_packet_uncleaned_categories, names=names)\n",
    "                dataset = noise_generator(dataset, names)\n",
    "                name = \"uncleaned_categories-random\"\n",
    "            elif noise == 2:\n",
    "                csv_list = [path_to_per_packet_uncleaned_categories, path_to_iot_noise_cleaned]\n",
    "                dataset = combine_csv(csv_list, names)\n",
    "                name = \"uncleaned_categories-cleaned_iot\"\n",
    "            elif noise == 3:\n",
    "                csv_list = [path_to_per_packet_uncleaned_categories, path_to_iot_noise_uncleaned]\n",
    "                dataset = combine_csv(csv_list, names)\n",
    "                name = \"uncleaned_categories-uncleaned_iot\"\n",
    "            elif noise == 4:\n",
    "                csv_list = [path_to_per_packet_uncleaned_categories, path_to_network_noise_cleaned]\n",
    "                dataset = combine_csv(csv_list, names)\n",
    "                name = \"uncleaned_categories-cleaned_network\"\n",
    "            elif noise == 5:\n",
    "                csv_list = [path_to_per_packet_uncleaned_categories, path_to_network_noise_uncleaned]\n",
    "                dataset = combine_csv(csv_list, names)\n",
    "                name = \"uncleaned_categories-uncleaned_network\"\n",
    "        else:\n",
    "            dataset = read_csv(path_to_per_packet_uncleaned_categories, names=names)\n",
    "            name = \"uncleaned_categories\"\n",
    "    elif experiment_type == 3:\n",
    "        device = int(input(\"Select one of the following: \\n1. Plug \\n2. Bulb \\n3. Cam\"))\n",
    "        i_ni = int(input(\"Select one of the following: \\n1. Interaction \\n2. No Interaction\"))\n",
    "\n",
    "        if device == 1 and c_uc == 1 and i_ni == 1:\n",
    "            csv_list = os.listdir(path_to_same_plug_cleaned_interaction)\n",
    "            csv_list = [f\"{path_to_same_plug_cleaned_interaction + i}\" for i in csv_list]\n",
    "            dataset = combine_csv(csv_list, names)\n",
    "            name = \"plug-cleaned-interaction\"\n",
    "        elif device == 1 and c_uc == 1 and i_ni == 2:\n",
    "            csv_list = os.listdir(path_to_same_plug_cleaned_no_interaction)\n",
    "            csv_list = [f\"{path_to_same_plug_cleaned_no_interaction + i}\" for i in csv_list]\n",
    "            dataset = combine_csv(csv_list, names)\n",
    "            name = \"plug-cleaned-no_interaction\"\n",
    "        elif device == 1 and c_uc == 2 and i_ni == 1:\n",
    "            csv_list = os.listdir(path_to_same_plug_uncleaned_interaction)\n",
    "            csv_list = [f\"{path_to_same_plug_uncleaned_interaction + i}\" for i in csv_list]\n",
    "            dataset = combine_csv(csv_list, names)\n",
    "            name = \"plug-uncleaned-interaction\"\n",
    "        elif device == 1 and c_uc == 2 and i_ni == 2:\n",
    "            csv_list = os.listdir(path_to_same_plug_uncleaned_no_interaction)\n",
    "            csv_list = [f\"{path_to_same_plug_uncleaned_no_interaction + i}\" for i in csv_list]\n",
    "            dataset = combine_csv(csv_list, names)\n",
    "            name = \"plug-uncleaned-no_interaction\"\n",
    "        elif device == 2 and c_uc == 1 and i_ni == 1:\n",
    "            csv_list = os.listdir(path_to_same_bulb_cleaned_interaction)\n",
    "            csv_list = [f\"{path_to_same_bulb_cleaned_interaction + i}\" for i in csv_list]\n",
    "            dataset = combine_csv(csv_list, names)\n",
    "            name = \"bulb-cleaned-interaction\"\n",
    "        elif device == 2 and c_uc == 1 and i_ni == 2:\n",
    "            csv_list = os.listdir(path_to_same_bulb_cleaned_no_interaction)\n",
    "            csv_list = [f\"{path_to_same_bulb_cleaned_no_interaction + i}\" for i in csv_list]\n",
    "            dataset = combine_csv(csv_list, names)\n",
    "            name = \"bulb-cleaned-no_interaction\"\n",
    "        elif device == 2 and c_uc == 2 and i_ni == 1:\n",
    "            csv_list = os.listdir(path_to_same_bulb_uncleaned_interaction)\n",
    "            csv_list = [f\"{path_to_same_bulb_uncleaned_interaction + i}\" for i in csv_list]\n",
    "            dataset = combine_csv(csv_list, names)\n",
    "            name = \"bulb-uncleaned-interaction\"\n",
    "        elif device == 2 and c_uc == 2 and i_ni == 2:\n",
    "            csv_list = os.listdir(path_to_same_bulb_uncleaned_no_interaction)\n",
    "            csv_list = [f\"{path_to_same_bulb_uncleaned_no_interaction + i}\" for i in csv_list]\n",
    "            dataset = combine_csv(csv_list, names)\n",
    "            name = \"bulb-uncleaned-no_interaction\"\n",
    "        elif device == 3 and c_uc == 1 and i_ni == 1:\n",
    "            csv_list = os.listdir(path_to_same_cam_cleaned_interaction)\n",
    "            csv_list = [f\"{path_to_same_cam_cleaned_interaction + i}\" for i in csv_list]\n",
    "            dataset = combine_csv(csv_list, names)\n",
    "            name = \"cam-cleaned-interaction\"\n",
    "        elif device == 3 and c_uc == 1 and i_ni == 2:\n",
    "            csv_list = os.listdir(path_to_same_cam_cleaned_no_interaction)\n",
    "            csv_list = [f\"{path_to_same_cam_cleaned_no_interaction + i}\" for i in csv_list]\n",
    "            dataset = combine_csv(csv_list, names)\n",
    "            name = \"cam-cleaned-no_interaction\"\n",
    "        elif device == 3 and c_uc == 2 and i_ni == 1:\n",
    "            csv_list = os.listdir(path_to_same_cam_uncleaned_interaction)\n",
    "            csv_list = [f\"{path_to_same_cam_uncleaned_interaction + i}\" for i in csv_list]\n",
    "            dataset = combine_csv(csv_list, names)\n",
    "            name = \"cam-uncleaned-interaction\"\n",
    "        elif device == 3 and c_uc == 2 and i_ni == 2:\n",
    "            csv_list = os.listdir(path_to_same_cam_uncleaned_no_interaction)\n",
    "            csv_list = [f\"{path_to_same_cam_uncleaned_no_interaction + i}\" for i in csv_list]\n",
    "            dataset = combine_csv(csv_list, names)\n",
    "            name = \"cam-uncleaned-no_interaction\"\n",
    "\n",
    "    elif experiment_type == 4:\n",
    "\n",
    "        accum = int(input(\"Select one of the following accumulator sizes: \\n128 \\n256 \\n512 \\n1024\"))\n",
    "\n",
    "        if accum == 128:\n",
    "            names = ['dim1','dim2','dim3','dim4','dim5','dim6','dim7','dim8','dim9','dim10','dim11','dim12','dim13','dim14','dim15','dim16','class']\n",
    "        elif accum == 256:\n",
    "            names = ['dim1','dim2','dim3','dim4','dim5','dim6','dim7','dim8','dim9','dim10','dim11','dim12','dim13','dim14','dim15','dim16','dim17','dim18','dim19','dim20','dim21','dim22','dim23','dim24','dim25','dim26','dim27','dim28','dim29','dim30','dim31','dim32','class']\n",
    "        elif accum == 512:\n",
    "            names = ['dim1','dim2','dim3','dim4','dim5','dim6','dim7','dim8','dim9','dim10','dim11','dim12','dim13','dim14','dim15','dim16','dim17','dim18','dim19','dim20','dim21','dim22','dim23','dim24','dim25','dim26','dim27','dim28','dim29','dim30','dim31','dim32','dim33','dim34','dim35','dim36','dim37','dim38','dim39','dim40','dim41','dim42','dim43','dim44','dim45','dim46','dim47','dim48','dim49','dim50','dim51','dim52','dim53','dim54','dim55','dim56','dim57','dim58','dim59','dim60','dim61','dim62','dim63','dim64','class']\n",
    "        if accum == 1024:\n",
    "            names = ['dim1','dim2','dim3','dim4','dim5','dim6','dim7','dim8','dim9','dim10','dim11','dim12','dim13','dim14','dim15','dim16','dim17','dim18','dim19','dim20','dim21','dim22','dim23','dim24','dim25','dim26','dim27','dim28','dim29','dim30','dim31','dim32','dim33','dim34','dim35','dim36','dim37','dim38','dim39','dim40','dim41','dim42','dim43','dim44','dim45','dim46','dim47','dim48','dim49','dim50','dim51','dim52','dim53','dim54','dim55','dim56','dim57','dim58','dim59','dim60','dim61','dim62','dim63','dim64', 'dim65','dim66','dim67','dim68','dim69','dim70','dim71','dim72','dim73','dim74','dim75','dim76','dim77','dim78','dim79','dim80','dim81','dim82','dim83','dim84','dim85','dim86','dim87','dim88','dim89','dim90','dim91','dim92','dim93','dim94','dim95','dim96','dim97','dim98','dim99','dim100','dim101','dim102','dim103','dim104','dim105','dim106','dim107','dim108','dim109','dim110','dim111','dim112','dim113','dim114','dim115','dim116','dim117','dim118','dim119','dim120','dim121','dim122','dim123','dim124','dim125','dim126','dim127','dim128','class']\n",
    "\n",
    "        window = int(input(\"Select one of the following window sizes: \\n4 \\n5 \\n6\"))\n",
    "        if window == 4:\n",
    "            combo = int(input(\"Select one of the following combination sizes: \\n2 \\n3 \\n4\"))\n",
    "        elif window == 5:\n",
    "            combo = int(input(\"Select one of the following combination sizes: \\n2 \\n3 \\n4 \\n5\"))\n",
    "        elif window == 6:\n",
    "            combo = int(input(\"Select one of the following combination sizes: \\n2 \\n3 \\n4 \\n5 \\n6\"))\n",
    "\n",
    "        if c_uc == 1:\n",
    "            target_dir = f\"{path_to_simhash}{accum}/win_{window}/comb_{combo}/cleaned/\"\n",
    "            csv_list = os.listdir(target_dir)\n",
    "            csv_list = [f\"{target_dir + i}\" for i in csv_list]\n",
    "            name = f\"SimHash-{accum}-win_{window}-combo_{combo}-cleaned\"\n",
    "        elif c_uc == 2:\n",
    "            target_dir = f\"{path_to_simhash}{accum}/win_{window}/comb_{combo}/uncleaned/\"\n",
    "            csv_list = os.listdir(target_dir)\n",
    "            csv_list = [f\"{target_dir + i}\" for i in csv_list]\n",
    "            name = f\"SimHash-{accum}-win_{window}-combo_{combo}-uncleaned\"\n",
    "\n",
    "        dataset = combine_csv(csv_list, names)\n",
    "\n",
    "    elif experiment_type == 5:\n",
    "        hash_alg = int(input(\"Select on of the following hashing algorithms: \\n1. Nilsimsa \\n2.FlexHash\"))\n",
    "        noise = int(input(\"Select one of the following: \\n1. Random \\n2. IoT Cleaned \\n3. IoT Uncleaned \\n4. Network Cleaned \\n5. Network Uncleaned\"))\n",
    "        if hash_alg == 1:\n",
    "            device = int(input(\"Select one of the following: \\n1. Plug \\n2. Bulb \\n3. Cam\"))\n",
    "            i_ni = int(input(\"Select one of the following: \\n1. Interaction \\n2. No Interaction\"))\n",
    "            device_num = int(input(\"Select a device number (1-8): \"))\n",
    "\n",
    "            if device == 1 and c_uc == 1 and i_ni == 1:\n",
    "                csv_list = sorted(os.listdir(path_to_same_plug_cleaned_interaction))\n",
    "                csv_list = [path_to_same_plug_cleaned_interaction + csv_list[device_num-1]]\n",
    "\n",
    "                name = f\"plug-{device_num}-cleaned-interaction_100x\"\n",
    "            elif device == 1 and c_uc == 1 and i_ni == 2:\n",
    "                csv_list = sorted(os.listdir(path_to_same_plug_cleaned_no_interaction))\n",
    "                csv_list = [path_to_same_plug_cleaned_no_interaction + csv_list[device_num-1]]\n",
    "\n",
    "                name = f\"plug-{device_num}-cleaned-no_interaction_100x\"\n",
    "            elif device == 1 and c_uc == 2 and i_ni == 1:\n",
    "                csv_list = sorted(os.listdir(path_to_same_plug_uncleaned_interaction))\n",
    "                csv_list = [path_to_same_plug_uncleaned_interaction + csv_list[device_num-1]]\n",
    "\n",
    "                name = f\"plug-{device_num}-uncleaned-interaction_100x\"\n",
    "            elif device == 1 and c_uc == 2 and i_ni == 2:\n",
    "                csv_list = sorted(os.listdir(path_to_same_plug_uncleaned_no_interaction))\n",
    "                csv_list = [path_to_same_plug_uncleaned_no_interaction + csv_list[device_num-1]]\n",
    "\n",
    "                name = f\"plug-{device_num}-uncleaned-no_interaction_100x\"\n",
    "            elif device == 2 and c_uc == 1 and i_ni == 1:\n",
    "                csv_list = sorted(os.listdir(path_to_same_bulb_cleaned_interaction))\n",
    "                csv_list = [path_to_same_bulb_cleaned_interaction + csv_list[device_num-1]]\n",
    "\n",
    "                name = f\"bulb-{device_num}-cleaned-interaction_100x\"\n",
    "            elif device == 2 and c_uc == 1 and i_ni == 2:\n",
    "                csv_list = sorted(os.listdir(path_to_same_bulb_cleaned_no_interaction))\n",
    "                csv_list = [path_to_same_bulb_cleaned_no_interaction + csv_list[device_num-1]]\n",
    "\n",
    "                name = f\"bulb-{device_num}-cleaned-no_interaction_100x\"\n",
    "            elif device == 2 and c_uc == 2 and i_ni == 1:\n",
    "                csv_list = sorted(os.listdir(path_to_same_bulb_uncleaned_interaction))\n",
    "                csv_list = [path_to_same_bulb_uncleaned_interaction + csv_list[device_num-1]]\n",
    "\n",
    "                name = f\"bulb-{device_num}-uncleaned-interaction_100x\"\n",
    "            elif device == 2 and c_uc == 2 and i_ni == 2:\n",
    "                csv_list = sorted(os.listdir(path_to_same_bulb_uncleaned_no_interaction))\n",
    "                csv_list = [path_to_same_bulb_uncleaned_no_interaction + csv_list[device_num-1]]\n",
    "\n",
    "                name = f\"bulb-{device_num}-uncleaned-no_interaction_100x\"\n",
    "            elif device == 3 and c_uc == 1 and i_ni == 1:\n",
    "                csv_list = sorted(os.listdir(path_to_same_cam_cleaned_interaction))\n",
    "                csv_list = [path_to_same_cam_cleaned_interaction + csv_list[device_num-1]]\n",
    "\n",
    "                name = f\"cam-{device_num}-cleaned-interaction_100x\"\n",
    "            elif device == 3 and c_uc == 1 and i_ni == 2:\n",
    "                csv_list = sorted(os.listdir(path_to_same_cam_cleaned_no_interaction))\n",
    "                csv_list = [path_to_same_cam_cleaned_no_interaction + csv_list[device_num-1]]\n",
    "\n",
    "                name = f\"cam-{device_num}-cleaned-no_interaction_100x\"\n",
    "            elif device == 3 and c_uc == 2 and i_ni == 1:\n",
    "                csv_list = sorted(os.listdir(path_to_same_cam_uncleaned_interaction))\n",
    "                csv_list = [path_to_same_cam_uncleaned_interaction + csv_list[device_num-1]]\n",
    "\n",
    "                name = f\"cam-{device_num}-uncleaned-interaction_100x\"\n",
    "            elif device == 3 and c_uc == 2 and i_ni == 2:\n",
    "                csv_list = sorted(os.listdir(path_to_same_cam_uncleaned_no_interaction))\n",
    "                csv_list = [path_to_same_cam_uncleaned_no_interaction + csv_list[device_num-1]]\n",
    "\n",
    "                name = f\"cam-{device_num}-uncleaned-no_interaction_100x\"\n",
    "\n",
    "            if noise == 2:\n",
    "                csv_list.append(path_to_iot_noise_cleaned)\n",
    "                name += \"-cleaned_iot\"\n",
    "            elif noise == 3:\n",
    "                csv_list.append(path_to_iot_noise_uncleaned)\n",
    "                name += \"-uncleaned_iot\"\n",
    "            elif noise == 4:\n",
    "                csv_list.append(path_to_network_noise_cleaned)\n",
    "                name += \"-cleaned_network\"\n",
    "            elif noise == 5:\n",
    "                csv_list.append(path_to_network_noise_uncleaned)\n",
    "                name += \"-uncleaned_network\"\n",
    "\n",
    "            dataset = combine_csv(csv_list, names)\n",
    "\n",
    "            if noise == 1:\n",
    "                dataset = noise_generator(dataset, names, False)\n",
    "                name += \"-random\"\n",
    "\n",
    "            print(csv_list, name)\n",
    "\n",
    "    return dataset, name\n",
    "\n",
    "dataset, name_of_current_data = get_dataset()\n",
    "collected = gc.collect()\n",
    "print(\"Garbage collector: collected %d objects.\" % (collected))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Total samples in cam-cleaned-no_interaction: 322434 ***\n",
      "*** Samples for device: cam-1 in cam-cleaned-no_interaction: 40774 (0.12645688730096702%) ***\n",
      "*** Samples for device: cam-2 in cam-cleaned-no_interaction: 40506 (0.12562570944751483%) ***\n",
      "*** Samples for device: cam-3 in cam-cleaned-no_interaction: 40396 (0.12528455435841132%) ***\n",
      "*** Samples for device: cam-4 in cam-cleaned-no_interaction: 39520 (0.12256771928518705%) ***\n",
      "*** Samples for device: cam-5 in cam-cleaned-no_interaction: 40618 (0.12597306735642022%) ***\n",
      "*** Samples for device: cam-6 in cam-cleaned-no_interaction: 40446 (0.1254396248534584%) ***\n",
      "*** Samples for device: cam-7 in cam-cleaned-no_interaction: 40299 (0.12498371759802006%) ***\n",
      "*** Samples for device: cam-8 in cam-cleaned-no_interaction: 39875 (0.12366871980002109%) ***\n",
      "Garbage collector: collected 0 objects.\n",
      "*** Dataset Loaded ***\n"
     ]
    }
   ],
   "source": [
    "print(f\"*** Total samples in {name_of_current_data}: {len(dataset.index)} ***\")\n",
    "for device_name in sorted(dataset[\"class\"].unique()):\n",
    "    num_samples = len((dataset[dataset[\"class\"] == device_name]).index)\n",
    "    print(f\"*** Samples for device: {device_name} in {name_of_current_data}: {num_samples} ({num_samples/dataset.shape[0]}%) ***\")\n",
    "\n",
    "# classes_to_remove = [\"light-4\", \"light-5\", \"light-6\", \"light-7\", \"light-8\",]\n",
    "# for item in classes_to_remove:\n",
    "#     dataset = remove_class(item, dataset)\n",
    "#     dataset.dropna(inplace=True)\n",
    "\n",
    "# Uncomment this line to take only a portion of the data\n",
    "# dataset = dataset.head(len(dataset.index)//10)\n",
    "\n",
    "# x is the entire dataframe except for the class column\n",
    "x = dataset.drop(['class'], axis=1)\n",
    "\n",
    "# y_original is an unaltered list of all values in the class column\n",
    "y_original = dataset['class'].values.tolist()\n",
    "\n",
    "# y is a dataframe of only the class column and the values have been converted to numeric representation\n",
    "y = dataset['class']\n",
    "counter = 0\n",
    "y_temp = dataset['class'].tolist()\n",
    "for unique_value in sorted(y.unique()):\n",
    "    for index, value in enumerate(y):\n",
    "        if value == unique_value:\n",
    "            y_temp[index] = counter\n",
    "    counter += 1\n",
    "dataset[\"class\"] = y_temp\n",
    "y = dataset['class']\n",
    "labels_numeric = dataset['class'].unique()\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x.values, y.values, test_size=.2, stratify=y.values)\n",
    "\n",
    "del dataset\n",
    "# del x\n",
    "# del y\n",
    "del y_original\n",
    "del y_temp\n",
    "del labels_numeric\n",
    "collected = gc.collect()\n",
    "print(\"Garbage collector: collected %d objects.\" % (collected))\n",
    "\n",
    "print(\"*** Dataset Loaded ***\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# param_grid_HGBC = {\"learning_rate\": [0.01, 0.001, .1], \"max_leaf_nodes\": [None, 31, 50, 100], \"max_depth\": [None, 8, 16, 32, 64, 128], \"min_samples_leaf\": [5, 20, 100], \"l2_regularization\": [0, .1, .5, 1]}\n",
    "# HGBC = ensemble.HistGradientBoostingClassifier()\n",
    "# clf_HGBC = GridSearchCV(HGBC, param_grid_HGBC, n_jobs=20).fit(x.values, y.values)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# param_grid_RFC = {\"n_estimators\": [50, 100, 200, 500, 1000], \"min_samples_leaf\": [1, 5, 10, 20, 50, 100], \"min_samples_split\": [1, 2, 5, 10, 20, 50, 100], \"l2_regularization\": [.1, .3, .5, .7, 1], \"max_depth\": [5, 10, 30, 50, 100, 200]}\n",
    "# RFC = ensemble.RandomForestClassifier()\n",
    "# clf_RFC = GridSearchCV(RFC, param_grid_RFC, n_jobs=20).fit(x.values, y.values)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Begin Training and Evaluating 1 ***\n",
      "*** Finished Training and Evaluating 1 ***\n",
      "Dataset Name: cam-cleaned-no_interaction\n",
      "Runtime: 4.64546799659729\n",
      "Accuracy: 0.12176445412084333\n",
      "Balanced Accuracy: 0.12128887669066196\n",
      "Weighted F1: 0.11282157223050782\n"
     ]
    }
   ],
   "source": [
    "# Spot Check Algorithms\n",
    "# x = [1000 for i in range(100)]\n",
    "# x = (* x,)\n",
    "\n",
    "models = []\n",
    "# models.append((1, ensemble.BaggingClassifier(base_estimator=ensemble.RandomForestClassifier(max_depth=10), n_estimators=50, bootstrap_features=True, n_jobs=16)))\n",
    "# models.append((1, ensemble.AdaBoostClassifier(base_estimator=ensemble.RandomForestClassifier(), n_estimators=50)))\n",
    "# models.append((2, ensemble.AdaBoostClassifier(base_estimator=ensemble.RandomForestClassifier(max_depth=10), n_estimators=50)))\n",
    "# models.append((2, MLPClassifier()))\n",
    "models.append((1, ensemble.HistGradientBoostingClassifier()))\n",
    "# models.append((2, ensemble.HistGradientBoostingClassifier(max_depth=32)))\n",
    "# models.append((3, ensemble.HistGradientBoostingClassifier(max_depth=128)))\n",
    "# models.append((2, ensemble.RandomForestClassifier()))\n",
    "# models.append((3, ensemble.AdaBoostClassifier(base_estimator=ensemble.HistGradientBoostingClassifier(), n_estimators=50)))\n",
    "# models.append((4, ensemble.AdaBoostClassifier(base_estimator=ensemble.HistGradientBoostingClassifier(max_depth=8), n_estimators=50)))\n",
    "# models.append((5, ensemble.AdaBoostClassifier(base_estimator=ensemble.HistGradientBoostingClassifier(max_depth=16), n_estimators=50)))\n",
    "# models.append((6, ensemble.AdaBoostClassifier(base_estimator=ensemble.HistGradientBoostingClassifier(max_depth=24), n_estimators=50)))\n",
    "# models.append((7, ensemble.AdaBoostClassifier(base_estimator=ensemble.HistGradientBoostingClassifier(max_depth=32), n_estimators=50)))\n",
    "# models.append((8, ensemble.AdaBoostClassifier(base_estimator=ensemble.HistGradientBoostingClassifier(), n_estimators=100)))\n",
    "# models.append((9, ensemble.AdaBoostClassifier(base_estimator=ensemble.HistGradientBoostingClassifier(max_depth=8), n_estimators=100)))\n",
    "# models.append((10, ensemble.AdaBoostClassifier(base_estimator=ensemble.HistGradientBoostingClassifier(max_depth=16), n_estimators=100)))\n",
    "# models.append((11, ensemble.AdaBoostClassifier(base_estimator=ensemble.HistGradientBoostingClassifier(max_depth=24), n_estimators=100)))\n",
    "# models.append((12, ensemble.AdaBoostClassifier(base_estimator=ensemble.HistGradientBoostingClassifier(max_depth=32), n_estimators=100)))\n",
    "# models.append((4, ensemble.AdaBoostClassifier(base_estimator=ensemble.HistGradientBoostingClassifier(max_depth=128), n_estimators=50)))\n",
    "# models.append((4, ensemble.AdaBoostClassifier(base_estimator=ensemble.HistGradientBoostingClassifier(max_depth=8), n_estimators=50)))\n",
    "# models.append((5, ensemble.AdaBoostClassifier(base_estimator=ensemble.HistGradientBoostingClassifier(max_depth=32), n_estimators=50)))\n",
    "# models.append((4, ensemble.AdaBoostClassifier(base_estimator=ensemble.HistGradientBoostingClassifier(max_depth=1), n_estimators=32)))\n",
    "# models.append((4, ensemble.AdaBoostClassifier(base_estimator=ensemble.HistGradientBoostingClassifier(max_depth=1), n_estimators=500)))\n",
    "# models.append((5, ensemble.AdaBoostClassifier(base_estimator=ensemble.HistGradientBoostingClassifier(max_depth=1), n_estimators=1000)))\n",
    "# models.append((6, ensemble.AdaBoostClassifier(base_estimator=ensemble.HistGradientBoostingClassifier(max_depth=1), n_estimators=10000)))\n",
    "# models.append((4, ensemble.AdaBoostClassifier(base_estimator=ensemble.HistGradientBoostingClassifier(l2_regularization=0.2, learning_rate=0.2, min_samples_leaf=100), n_estimators=50)))\n",
    "# models.append((1, ensemble.GradientBoostingClassifier(max_depth=10)))\n",
    "# models.append((1, ensemble.GradientBoostingClassifier(max_depth=100)))\n",
    "\n",
    "# evaluate each model\n",
    "for model_name, model in models:\n",
    "    print(f\"*** Begin Training and Evaluating {model_name} ***\")\n",
    "    start_time = time.time()\n",
    "    # print(y_train.shape)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # print(f\"*** {model_name} Trained ***\")\n",
    "\n",
    "    # y_pred = model.predict(x_test)\n",
    "    # y_probas = model.predict_proba(x_test)\n",
    "\n",
    "    # weighted_acc_dict = {}\n",
    "    # for index, label in enumerate(y_test):\n",
    "    #     weighted_acc_dict[f\"{label}_count\"] += 1\n",
    "    #     if y_pred[index] == label:\n",
    "    #         weighted_acc_dict[label] += 1\n",
    "\n",
    "    # total_accuracy = accuracy_score(y_test, y_pred)\n",
    "    # total_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    # print(f\"Accuracy: {total_accuracy}\")\n",
    "    # print(f\"F1: {total_f1}\")\n",
    "\n",
    "    # ******************** #\n",
    "    # Cross Validation\n",
    "    # ******************** #\n",
    "    cross_val_results = cross_validate(model, x.values, y.values, cv=7, scoring=['accuracy', 'balanced_accuracy', 'f1_weighted'], n_jobs=7)\n",
    "    print(f\"*** Finished Training and Evaluating {model_name} ***\")\n",
    "    print(f\"Dataset Name: {name_of_current_data}\")\n",
    "    print(f\"Runtime: {time.time() - start_time}\")\n",
    "    print(f\"Accuracy: {mean(cross_val_results['test_accuracy'])}\")\n",
    "    print(f\"Balanced Accuracy: {mean(cross_val_results['test_balanced_accuracy'])}\")\n",
    "    # print(f\"F1: {mean(cross_val_results['test_f1'])}\")\n",
    "    print(f\"Weighted F1: {mean(cross_val_results['test_f1_weighted'])}\")\n",
    "\n",
    "    # wandb.log({f\"Total accuracy TSR on {name_of_current_data}\": total_accuracy,\n",
    "    #            \"Dataset\": name_of_current_data,\n",
    "    #            \"Num Samples\": dataset.shape[0]})\n",
    "    # wandb.log({f\"Total precision TSR on {name_of_current_data}\": total_precision,\n",
    "    #            \"Dataset\": name_of_current_data,\n",
    "    #            \"Num Samples\": dataset.shape[0]})\n",
    "    # wandb.log({f\"Total recall TSR on {name_of_current_data}\": total_recall,\n",
    "    #            \"Dataset\": name_of_current_data,\n",
    "    #            \"Num Samples\": dataset.shape[0]})\n",
    "    # wandb.log({f\"Total f1 TSR on {name_of_current_data}\": total_f1,\n",
    "    #            \"Dataset\": name_of_current_data,\n",
    "    #            \"Num Samples\": dataset.shape[0]})\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# def draw_umap(data, n_neighbors, min_dist, n_components, metric, title, save_path):\n",
    "#     umap_reducer = umap.UMAP(\n",
    "#         n_neighbors=n_neighbors,\n",
    "#         min_dist=min_dist,\n",
    "#         n_components=n_components,\n",
    "#         metric=metric\n",
    "#     )\n",
    "#\n",
    "#     umap_embedding = umap_reducer.fit_transform(data)\n",
    "#\n",
    "#     fig = plt.figure(figsize=(5, 5))\n",
    "#     if n_components == 1:\n",
    "#         umap_df = pd.DataFrame(umap_embedding, columns=[\"dim1\"])\n",
    "#         umap_df[\"class\"] = y_train\n",
    "#\n",
    "#         ax = fig.add_subplot(111)\n",
    "#         ax.scatter(umap_df[\"dim1\"].values, range(len(umap_df.index)), c=umap_df[\"class\"].values, s=1)\n",
    "#     elif n_components == 2:\n",
    "#         umap_df = pd.DataFrame(umap_embedding, columns=[\"dim1\", \"dim2\"])\n",
    "#         umap_df[\"class\"] = y_train\n",
    "#\n",
    "#         ax = fig.add_subplot(111)\n",
    "#         ax.scatter(umap_df[\"dim1\"].values, umap_df[\"dim2\"].values, c=umap_df[\"class\"].values, s=1)\n",
    "#     else:\n",
    "#         umap_df = pd.DataFrame(umap_embedding, columns=[\"dim1\", \"dim2\", \"dim3\"])\n",
    "#         umap_df[\"class\"] = y_train\n",
    "#         ax = fig.add_subplot(111, projection='3d')\n",
    "#         ax.scatter(umap_df[\"dim1\"].values, umap_df[\"dim2\"].values,umap_df[\"dim3\"].values, c=umap_df[\"class\"].values, s=1)\n",
    "#\n",
    "#     plt.title(title, fontsize=8)\n",
    "#\n",
    "#     plt.savefig(save_path, dpi=1200)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# # n_neighbors adjusts the UMAP's attention to local structure vs. global relationships\n",
    "# # min_dist adjusts how close umap is allowed to place points together\n",
    "# if not os.path.isdir(f\"../figures/{name_of_current_data}/\"):\n",
    "#     os.mkdir(f\"../figures/{name_of_current_data}/\")\n",
    "#\n",
    "# num_generations = 2\n",
    "# for i in tqdm(range(3)):\n",
    "#     for j in range(num_generations):\n",
    "#         n_neighbors = 15\n",
    "#         min_dist = 0.1\n",
    "#         n_components = i+1\n",
    "#         metric = \"euclidean\"\n",
    "#         # metric = \"minkowski\"\n",
    "#\n",
    "#         title = f\"{name_of_current_data}_{n_neighbors}_{min_dist}_{n_components}_{metric}\"\n",
    "#         save_path = f\"../figures/{name_of_current_data}/{n_components}d_{j+1}.png\"\n",
    "#         # save_path = f\"/home/nthom/Documents/nilsimsa_vis/{n_components}d_{j+1}.png\"\n",
    "#         draw_umap(x_train, n_neighbors, min_dist, n_components, metric, title, save_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
