# distributed training
nodes: 1
gpus: 1 # I recommend always assigning 1 GPU to 1 node
workers: 12

# train options
seed: 256 # sacred handles automatic seeding when passed in the config
batch_size: 1024
learning_rate: 0.00003
start_epoch: 0
epochs: 20
pretrain: False
train: True
val_only: False
test: False
save: True
reload: False

# dataset options
dataset_dir: "../../data/"
dataset: "clddr_random_noise_devices/"
train_size: 0.8
val_size: 0.1
test_size: 0.1
print_num_samples_bool: False

# model options
model: "MLP"
in_features: 32
num_layers: 1
num_nodes_per_layer: 500
out_features: 128

# loss options
optimizer: "Adam"
weight_decay: 1.0e-6

# save options
save_name: "triple_loss_mlp"

# reload options
model_path: "./checkpoints/"
model_file: "triple_loss_mlp"

dataset_column_names: ['dim1', 'dim2', 'dim3', 'dim4', 'dim5', 'dim6', 'dim7', 'dim8', 'dim9', 'dim10', 'dim11','dim12',
                       'dim13', 'dim14', 'dim15', 'dim16', 'dim17', 'dim18', 'dim19', 'dim20', 'dim21','dim22', 'dim23',
                       'dim24', 'dim25', 'dim26', 'dim27', 'dim28', 'dim29', 'dim30', 'dim31', 'dim32', 'class']
