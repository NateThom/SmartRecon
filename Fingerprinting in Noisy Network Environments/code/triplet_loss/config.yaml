# distributed training
nodes: 1
gpus: 1 # I recommend always assigning 1 GPU to 1 node
workers: 8

# train options
seed: 512 # sacred handles automatic seeding when passed in the config
batch_size: 8192
learning_rate: 0.001
start_epoch: 0
epochs: 10
pretrain: False
train: True
val_only: False
test: False

# dataset options
dataset_dir: "../../data/"
dataset: "clddr_random_noise_devices/"
train_size: 0.8
val_size: 0.1
test_size: 0.1
print_num_samples_bool: False

# model options
model: "MLP"
in_features: 32
num_layers_hidden_layers: 5
num_nodes_per_layer: 1024
out_features: 19

# loss options
alpha: 0.2
optimizer: "Adam"
weight_decay: 1.0e-6

# save options
save: False
save_name: "triple_loss_mlp"

# reload options
reload: False
model_path: "./checkpoints/"
model_file: "triple_loss_mlp"

dataset_column_names: ['dim1', 'dim2', 'dim3', 'dim4', 'dim5', 'dim6', 'dim7', 'dim8', 'dim9', 'dim10', 'dim11','dim12',
                       'dim13', 'dim14', 'dim15', 'dim16', 'dim17', 'dim18', 'dim19', 'dim20', 'dim21','dim22', 'dim23',
                       'dim24', 'dim25', 'dim26', 'dim27', 'dim28', 'dim29', 'dim30', 'dim31', 'dim32', 'class']
